import pandas as pd
from openai import OpenAI
import json
import time
import os
import sys
import io
from tqdm import tqdm

# --- 1. åˆå§‹åŒ–Client (ä¸ä½ ä¹‹å‰çš„ä»£ç ç›¸åŒ) ---
api_key = os.getenv("DASHSCOPE_API_KEY")
if api_key is None:
    print("è­¦å‘Šï¼šæœªåœ¨ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ° DASHSCOPE_API_KEYï¼Œå°†ä½¿ç”¨ä»£ç ä¸­ç¡¬ç¼–ç çš„Keyã€‚", file=sys.stderr)
    api_key = "sk-4fcc85e2509649198bdcafa4e985ce6e" 

client = OpenAI(
    api_key=api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# --- 2. å¤šä»»åŠ¡æ‹†è§£å‡½æ•° (ä¸æˆ‘ä»¬ä¹‹å‰å®Œå–„çš„ç‰ˆæœ¬ç›¸åŒ) ---
def decompose_query(user_query):
    # ã€å·²å‡çº§ã€‘è¿™æ˜¯æ”¯æŒå¤šä»»åŠ¡æ‹†è§£çš„Promptæ¨¡æ¿
    prompt_template = """
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„å¤šä»»åŠ¡æ‹†è§£ä¸“å®¶ã€‚ä½ çš„ä¸“é•¿æ˜¯å°†ç”¨æˆ·å¯èƒ½åŒ…å«å¤šä¸ªæ„å›¾çš„å¤æ‚ã€å£è¯­åŒ–çš„æŸ¥è¯¢ï¼Œåˆ†è§£æˆä¸€ä¸ªæˆ–å¤šä¸ªæ¸…æ™°ã€ç‹¬ç«‹ã€å¯æ‰§è¡Œçš„å­ä»»åŠ¡åˆ—è¡¨ã€‚

# ä»»åŠ¡
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ï¼ˆqueryï¼‰ï¼Œå°†å…¶æ‹†è§£æˆä¸€ä¸ªJSONæ ¼å¼çš„å­ä»»åŠ¡åˆ—è¡¨ã€‚æ¯ä¸ªå­ä»»åŠ¡éƒ½å¿…é¡»æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¢«åç»­ç¨‹åºç›´æ¥æ‰§è¡Œã€‚

# è¾“å‡ºè§„èŒƒ
1.  æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONæ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰ã€‚
2.  å¦‚æœåŸå§‹queryåªåŒ…å«ä¸€ä¸ªä»»åŠ¡ï¼Œåˆ™åˆ—è¡¨ä¸­åªæœ‰ä¸€ä¸ªJSONå¯¹è±¡ã€‚
3.  å¦‚æœåŸå§‹queryåŒ…å«å¤šä¸ªä»»åŠ¡ï¼Œåˆ™åˆ—è¡¨ä¸­æœ‰å¤šä¸ªJSONå¯¹è±¡ã€‚
4.  å¦‚æœåŸå§‹queryä¸åŒ…å«ä»»ä½•å¯è¯†åˆ«çš„ä»»åŠ¡ï¼Œåˆ™è¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨ `[]`ã€‚
5.  åˆ—è¡¨ä¸­çš„æ¯ä¸ªJSONå¯¹è±¡éƒ½å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    *   `app_name`: å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ï¼š[æŠ–éŸ³, æŠ–éŸ³æé€Ÿç‰ˆ, å¿«æ‰‹, å¿«æ‰‹æé€Ÿç‰ˆ, æ‹¼å¤šå¤š, æ·˜å®, äº¬ä¸œ, å¤©çŒ«, é—²é±¼, æŠ–éŸ³ç«å±±ç‰ˆ, é˜¿é‡Œå·´å·´, å”¯å“ä¼š, å¾—ç‰©, è½¬è½¬]ã€‚æ³¨æ„è¯†åˆ«åˆ«åã€‚
    *   `category`: å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ï¼š[ç­¾åˆ°, æ”¶è—, æœç´¢, ç‰©æµ, è®¢å•, å‘ç¥¨, è´­ç‰©è½¦, å®¢æœ, å¯åŠ¨]ã€‚
    *   `decomposed_query`: é‡æ–°ç”Ÿæˆçš„ã€æ¸…æ™°çš„ã€é’ˆå¯¹å•ä¸ªä»»åŠ¡çš„æŒ‡ä»¤æ€§æŸ¥è¯¢ã€‚

# æ€è€ƒè¿‡ç¨‹
è¯·éµå¾ªä»¥ä¸‹æ€è€ƒè¿‡ç¨‹æ¥å®Œæˆä»»åŠ¡ï¼š
1.  ä»”ç»†é˜…è¯»ç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­å®ƒåŒ…å«ä¸€ä¸ªè¿˜æ˜¯å¤šä¸ªæ½œåœ¨çš„ä»»åŠ¡ç‚¹ï¼ˆä¾‹å¦‚ï¼Œå¤šä¸ªåŠ¨ä½œæˆ–å¤šä¸ªå¯¹è±¡ï¼‰ã€‚
2.  è¯†åˆ«æŸ¥è¯¢ä¸­çš„å…±äº«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚Appåç§°ï¼Œå®ƒå¯èƒ½é€‚ç”¨äºæ‰€æœ‰å­ä»»åŠ¡ã€‚
3.  å¯¹äºæ¯ä¸€ä¸ªè¯†åˆ«å‡ºçš„ä»»åŠ¡ç‚¹ï¼š
    a. ç¡®å®šå…¶`category`ï¼ˆæ„å›¾ï¼‰ã€‚
    b. ç¡®å®šå…¶`app_name`ï¼ˆå®ä½“ï¼‰ï¼Œåº”ç”¨å…±äº«ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚
    c. æå–ä»»åŠ¡çš„å…³é”®å‚æ•°ï¼ˆä¾‹å¦‚æœç´¢çš„ç‰©å“ã€æŸ¥çœ‹çš„è®¢å•ç±»å‹ç­‰ï¼‰ã€‚
    d. åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€å¥æ¸…æ™°ã€ç‹¬ç«‹çš„`decomposed_query`ã€‚
4.  å°†æ‰€æœ‰æ‹†è§£å‡ºçš„å­ä»»åŠ¡å¯¹è±¡ç»„åˆæˆä¸€ä¸ªJSONåˆ—è¡¨ã€‚

# ç¤ºä¾‹
---
query: "ç”¨æ·˜å®æŸ¥çœ‹ç«è½¦å’Œæœºç¥¨"
JSONè¾“å‡º:
[
  {{
    "app_name": "æ·˜å®",
    "category": "æœç´¢",
    "decomposed_query": "åœ¨æ·˜å®ä¸­æœç´¢ç«è½¦ç¥¨"
  }},
  {{
    "app_name": "æ·˜å®",
    "category": "æœç´¢",
    "decomposed_query": "åœ¨æ·˜å®ä¸­æœç´¢æœºç¥¨"
  }}
]
---
query: "å¸®æˆ‘åœ¨äº¬ä¸œçœ‹ä¸€ä¸‹è´­ç‰©è½¦ï¼Œé¡ºä¾¿æŸ¥ä¸‹æ˜¨å¤©çš„è®¢å•ç‰©æµ"
JSONè¾“å‡º:
[
  {{
    "app_name": "äº¬ä¸œ",
    "category": "è´­ç‰©è½¦",
    "decomposed_query": "åœ¨äº¬ä¸œä¸­æŸ¥çœ‹è´­ç‰©è½¦"
  }},
  {{
    "app_name": "äº¬ä¸œ",
    "category": "ç‰©æµ",
    "decomposed_query": "åœ¨äº¬ä¸œä¸­æŸ¥çœ‹æ˜¨å¤©çš„è®¢å•ç‰©æµ"
  }}
]
---
query: "å¿«å‘Šè¯‰æˆ‘æŠ–éŸ³æé€Ÿç‰ˆç­¾åˆ°é¢†é‡‘å¸çš„ä»»åŠ¡åœ¨å“ªï¼"
JSONè¾“å‡º:
[
  {{
    "app_name": "æŠ–éŸ³æé€Ÿç‰ˆ",
    "category": "ç­¾åˆ°",
    "decomposed_query": "åœ¨æŠ–éŸ³æé€Ÿç‰ˆä¸­æ‰“å¼€ç­¾åˆ°é¢†é‡‘å¸ä»»åŠ¡"
  }}
]
---
query: "å¸®æˆ‘æ‰“å¼€æŠ–éŸ³ï¼Œç„¶ååœ¨æ·˜å®æœä¸€ä¸‹æ–°å‡ºçš„æ‰‹æœºå£³"
JSONè¾“å‡º:
[
  {{
    "app_name": "æŠ–éŸ³",
    "category": "å¯åŠ¨",
    "decomposed_query": "æ‰“å¼€æŠ–éŸ³"
  }},
  {{
    "app_name": "æ·˜å®",
    "category": "æœç´¢",
    "decomposed_query": "åœ¨æ·˜å®ä¸­æœç´¢æ–°å‡ºçš„æ‰‹æœºå£³"
  }}
]
---

# å¼€å§‹ä»»åŠ¡
ç°åœ¨ï¼Œè¯·æ ¹æ®ä»¥ä¸Šè§„èŒƒï¼Œå¤„ç†ä»¥ä¸‹æ–°çš„ç”¨æˆ·æŸ¥è¯¢ã€‚

query: "{query}"
"""
    
    full_prompt = prompt_template.format(query=user_query)

    try:
        response = client.chat.completions.create(
            model="qwen2.5-7b-instruct",
            messages=[
                {"role": "system", "content": "ä½ å°†ä¸¥æ ¼æŒ‰ç…§æŒ‡ä»¤è¿›è¡Œå¤šä»»åŠ¡æ‹†è§£ï¼Œå¹¶åªè¾“å‡ºJSONæ ¼å¼çš„åˆ—è¡¨ã€‚ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚"},
                {"role": "user", "content": full_prompt}
            ],
            temperature=0,
            # top_p=0.8 # ä¸ temperature äºŒé€‰ä¸€
        )
        
        result_str = response.choices[0].message.content
        
        if result_str.strip().startswith("```json"):
            result_str = result_str.strip()[7:-3].strip()
            
        result_list = json.loads(result_str)
        return result_list

    except json.JSONDecodeError as e:
        print(f"\nJSONè§£ç å¤±è´¥: {e}", file=sys.stderr)
        print(f"æ¨¡å‹è¿”å›çš„åŸå§‹å­—ç¬¦ä¸²: '{result_str}'", file=sys.stderr)
        return [{"error": "JSON Error", "message": str(e)}]
    except Exception as e:
        print(f"\nå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", file=sys.stderr)
        return [{"error": "Unknown Error", "message": str(e)}]

# --- 3. æ‰¹é‡æµ‹è¯•ä¸è¯„ä¼°ä¸»ç¨‹åº ---
# --- 3. æ‰¹é‡æµ‹è¯•ä¸è¯„ä¼°ä¸»ç¨‹åº (ä¿®æ”¹ç‰ˆ) ---
def batch_test_and_evaluate(dataset_path):
    """
    è¯»å–CSVæ•°æ®é›†ï¼Œè¿›è¡Œæ‰¹é‡æµ‹è¯•ï¼Œå®æ—¶æ‰“å°æ¯ä¸ªæ ·æœ¬çš„ç»“æœï¼Œå¹¶æœ€ç»ˆè¯„ä¼°ã€‚
    """
    print(f"æ­£åœ¨ä» {dataset_path} åŠ è½½æ•°æ®é›†...")
    try:
        df = pd.read_csv(dataset_path)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {dataset_path}", file=sys.stderr)
        return

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç©ºçš„åˆ—åï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºå‡†é‡å‘½å
    first_valid_col = df.columns[0]
    col_map = {
        first_valid_col: 'category_label',
        df.columns[1]: 'app_name_label',
        df.columns[3]: 'final_query',
        df.columns[4]: 'is_train'
    }
    df = df.rename(columns=col_map)
    
    # ç­›é€‰å‡ºæµ‹è¯•é›† (is_train == 0)
    # æ³¨æ„ï¼šåŸå§‹æ•°æ®ä¸­is_trainåˆ—å¯èƒ½æ˜¯æµ®ç‚¹æ•°ï¼Œå…ˆå¤„ç†ä¸€ä¸‹
    df['is_train'] = pd.to_numeric(df['is_train'], errors='coerce').fillna(1).astype(int)
    test_df = df[df['is_train'] == 0].copy()
    
    print(f"æ•°æ®é›†åŠ è½½å®Œæ¯•ã€‚æ€»è¡Œæ•°: {len(df)}, æµ‹è¯•é›†è¡Œæ•°: {len(test_df)}")
    
    if len(test_df) == 0:
        print("æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ ·æœ¬ (is_train == 0)ã€‚")
        return

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    total_count = 0
    app_name_correct_count = 0
    category_correct_count = 0
    both_correct_count = 0
    errors = []

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå¹¶é…ç½®ä»¥æ–¹ä¾¿å®æ—¶æ‰“å°
    progress_bar = tqdm(test_df.iterrows(), total=test_df.shape[0], desc="æ‰¹é‡æµ‹è¯•ä¸­",
                        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')

    for index, row in progress_bar:
        total_count += 1
        query = row['final_query']
        app_name_label = row['app_name_label']
        category_label = row['category_label']

        # æ‰“å°å½“å‰æ­£åœ¨æµ‹è¯•çš„æ ·æœ¬
        print(f"\n--- æµ‹è¯•æ ·æœ¬ {total_count}/{len(test_df)} ---")
        print(f"  Query: {query}")
        
        # è°ƒç”¨æ¨¡å‹ (é‡è¯•é€»è¾‘ä¿æŒä¸å˜)
        tasks = [] # åˆå§‹åŒ–
        max_retries = 3
        for attempt in range(max_retries):
            try:
                tasks = decompose_query(query)
                break
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"\n  æŸ¥è¯¢æ—¶å‡ºé”™ï¼Œæ­£åœ¨é‡è¯•... é”™è¯¯: {e}", file=sys.stderr)
                    time.sleep(2)
                else:
                    print(f"\n  æŸ¥è¯¢å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè·³è¿‡ã€‚", file=sys.stderr)
                    tasks = [{"error": "Max retries exceeded"}]
        
        # è¯„ä¼°å¹¶å®æ—¶æ‰“å°ç»“æœ
        if tasks and isinstance(tasks, list) and len(tasks) > 0 and 'error' not in tasks[0]:
            predicted_task = tasks[0]
            predicted_app_name = predicted_task.get('app_name', 'None')
            predicted_category = predicted_task.get('category', 'None')
            
            app_name_match = (predicted_app_name == app_name_label)
            category_match = (predicted_category == category_label)

            # æ›´æ–°ç»Ÿè®¡æ•°æ®
            if app_name_match: app_name_correct_count += 1
            if category_match: category_correct_count += 1
            
            if app_name_match and category_match:
                both_correct_count += 1
                print("  âœ… ç»“æœå®Œå…¨æ­£ç¡®")
                print(f"    - é¢„æµ‹ä¸æœŸæœ›ä¸€è‡´: app='{predicted_app_name}', category='{predicted_category}'")
            else:
                print("  âŒ ç»“æœé”™è¯¯")
                error_details = {
                    "query": query,
                    "label": {"app_name": app_name_label, "category": category_label},
                    "prediction": {"app_name": predicted_app_name, "category": predicted_category}
                }
                errors.append(error_details)
                print(f"    - é¢„æµ‹: app_name='{predicted_app_name}' (åŒ¹é…: {app_name_match}), category='{predicted_category}' (åŒ¹é…: {category_match})")
                print(f"    - æœŸæœ›: app_name='{app_name_label}', category='{category_label}'")
        else:
            # æ¨¡å‹è¿”å›é”™è¯¯æˆ–ç©ºåˆ—è¡¨
            print("  âŒ æ¨¡å‹è¿”å›é”™è¯¯æˆ–æ— æ•ˆå“åº”")
            error_details = {
                "query": query,
                "label": {"app_name": app_name_label, "category": category_label},
                "prediction": {"error": tasks[0].get('error', 'Empty or invalid response') if tasks else "Empty list"}
            }
            errors.append(error_details)
            print(f"    - æœŸæœ›: app_name='{app_name_label}', category='{category_label}'")
            print(f"    - è¿”å›: {tasks}")
            
        time.sleep(0.5)

    # --- 4. æ‰“å°æœ€ç»ˆçš„è¯„ä¼°æŠ¥å‘Š (ä¿æŒä¸å˜) ---
    print("\n\n===================================")
    print("--- æ‰¹é‡æµ‹è¯•æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š ---")
    print("===================================")
    print(f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {total_count}")
    print("-" * 30)
    
    app_name_accuracy = (app_name_correct_count / total_count) * 100 if total_count > 0 else 0
    category_accuracy = (category_correct_count / total_count) * 100 if total_count > 0 else 0
    overall_accuracy = (both_correct_count / total_count) * 100 if total_count > 0 else 0

    print(f"App Name å‡†ç¡®ç‡: {app_name_accuracy:.2f}% ({app_name_correct_count}/{total_count})")
    print(f"Category å‡†ç¡®ç‡: {category_accuracy:.2f}% ({category_correct_count}/{total_count})")
    print(f"æ•´ä½“å‡†ç¡®ç‡ (ä¸¤è€…éƒ½æ­£ç¡®): {overall_accuracy:.2f}% ({both_correct_count}/{total_count})")
    print("-" * 30)

    if errors:
        print(f"\nå…±å‘ç° {len(errors)} ä¸ªé”™è¯¯æ¡ˆä¾‹ã€‚")
    else:
        print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹å‡é€šè¿‡ï¼")


if __name__ == "__main__":
    # ç¡®ä¿ä½ çš„CSVæ–‡ä»¶è·¯å¾„æ˜¯æ­£ç¡®çš„
    dataset_file_path = r"D:\Agent\data\generated_queries - 0704æ‰‹åŠ¨ç­›é€‰å.csv"
    
    if not os.path.exists(dataset_file_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æ•°æ®é›†æ–‡ä»¶ '{dataset_file_path}'", file=sys.stderr)
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–è€…å°†æ•°æ®é›†æ–‡ä»¶æ”¾åœ¨ä¸è„šæœ¬ç›¸åŒçš„ç›®å½•ä¸‹å¹¶å‘½åä¸º'dataset.csv'ã€‚", file=sys.stderr)
    else:
        batch_test_and_evaluate(dataset_file_path)