import pandas as pd
import os
import json
import ast
import numpy as np
import math
from tqdm import tqdm
from collections import defaultdict
import time
import itertools  # â˜…â˜…â˜… æ–°å¢ï¼šç”¨äºå‚æ•°ç½‘æ ¼æœç´¢ â˜…â˜…â˜…
from sklearn.model_selection import train_test_split # â˜…â˜…â˜… æ–°å¢ï¼šç”¨äºåˆ’åˆ†éªŒè¯é›† â˜…â˜…â˜…

# --- å¬å›å™¨å’Œè¯„æµ‹æ¨¡å— ---
from rank_bm25 import BM25Okapi
import jieba
from sklearn.metrics import roc_auc_score


# â˜…â˜…â˜… 1. å°†åˆ†è¯å™¨å¢å¼ºé€»è¾‘æ•´åˆåˆ° ToolRetriever ä¸­ â˜…â˜…â˜…
class ToolRetriever:
    def __init__(self, all_tools_corpus: list, all_tools_definitions: list, k1=1.5, b=0.75):
        self.tool_corpus = all_tools_corpus
        self.tool_definitions = all_tools_definitions
        self.k1 = k1
        self.b = b
        
        # å¢åŠ jiebaè‡ªå®šä¹‰è¯å…¸ï¼Œå¯ä»¥æé«˜å¯¹å·¥å…·åç§°å’Œæ ¸å¿ƒä¸šåŠ¡è¯çš„åˆ†è¯å‡†ç¡®æ€§
        self._add_jieba_words()
        
        tokenized_corpus = [self._tokenize(doc) for doc in self.tool_corpus]
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°åˆå§‹åŒ– BM25
        self.bm25 = BM25Okapi(tokenized_corpus, k1=self.k1, b=self.b)

    def _tokenize(self, text: str) -> list[str]:
        return jieba.lcut(text, cut_all=False)

    def _add_jieba_words(self):
        # æ·»åŠ å·¥å…·å
        for tool in self.tool_definitions:
            jieba.add_word(tool.get('name', ''), freq=100)
        # æ·»åŠ æ ¸å¿ƒä¸šåŠ¡è¯
        core_words = ["è´­ç‰©è½¦", "é‡‡è´­è½¦", "å¾…æ”¶è´§", "å¾…ä»˜æ¬¾", "æ”¶è—å¤¹", "é™ä»·", "ç­¾åˆ°", "ç§¯åˆ†", "å‘ç¥¨", "å¼€ç¥¨", "æŠ¥é”€å‡­è¯"]
        for word in core_words:
            jieba.add_word(word, freq=100)

    def retrieve_with_scores(self, query: str, top_k: int):
        tokenized_query = self._tokenize(query)
        all_scores = self.bm25.get_scores(tokenized_query)
        
        top_k_indices = all_scores.argsort()[-top_k:][::-1]
        retrieved = [self.tool_definitions[i] for i in top_k_indices if all_scores[i] > 0]
        
        return retrieved, all_scores

# --- (æ‰€æœ‰è¯„æµ‹å‡½æ•°ä¿æŒä¸å˜) ---
def _get_tool_names(tools: list) -> set:
    if not isinstance(tools, list): return set()
    return {tool.get('name') for tool in tools}

def calculate_recall_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    if not ground_truth: return 1.0
    retrieved_names_at_k = _get_tool_names(retrieved[:k])
    ground_truth_names = _get_tool_names(ground_truth)
    if not ground_truth_names: return 1.0
    return len(retrieved_names_at_k.intersection(ground_truth_names)) / len(ground_truth_names)
# ... å…¶ä»–è¯„æµ‹å‡½æ•°çœç•¥ä»¥ä¿æŒç®€æ´ï¼Œå®ƒä»¬ä¿æŒä¸å˜ ...
def calculate_completeness_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    if not ground_truth: return 1.0
    retrieved_names_at_k = _get_tool_names(retrieved[:k])
    ground_truth_names = _get_tool_names(ground_truth)
    return 1.0 if ground_truth_names.issubset(retrieved_names_at_k) else 0.0

def calculate_ndcg_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    ground_truth_names = _get_tool_names(ground_truth)
    if not ground_truth_names: return 1.0
    dcg = sum(1.0 / math.log2(i + 2) for i, tool in enumerate(retrieved[:k]) if tool.get('name') in ground_truth_names)
    idcg = sum(1.0 / math.log2(i + 2) for i in range(min(len(ground_truth_names), k)))
    return dcg / idcg if idcg > 0 else 0.0

def calculate_hit_ratio_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    retrieved_names = _get_tool_names(retrieved[:k])
    return 1.0 if retrieved_names & _get_tool_names(ground_truth) else 0.0

def calculate_average_precision_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    gt_names = _get_tool_names(ground_truth)
    if not gt_names: return 1.0
    hit_count = 0
    sum_prec = 0.0
    for i, tool in enumerate(retrieved[:k]):
        if tool.get('name') in gt_names:
            hit_count += 1
            sum_prec += hit_count / (i + 1)
    return sum_prec / len(gt_names)

def calculate_mrr_at_k(retrieved: list, ground_truth: list, k: int) -> float:
    gt_names = _get_tool_names(ground_truth)
    for i, tool in enumerate(retrieved[:k]):
        if tool.get('name') in gt_names:
            return 1.0 / (i + 1)
    return 0.0

def calculate_auc_for_query(all_scores: np.ndarray, tool_defs: list, ground_truth: list) -> float:
    gt_names = _get_tool_names(ground_truth)
    labels = [1 if t['name'] in gt_names else 0 for t in tool_defs]
    try:
        if len(set(labels)) < 2: return 0.5
        return roc_auc_score(labels, all_scores)
    except ValueError: return 0.5


# --- æƒå¨å·¥å…·å®šä¹‰æ¨¡å— (ä¿æŒä¸å˜) ---
def get_exact_tool_definitions():
    # ... (å‡½æ•°å†…å®¹ä¸å˜) ...
    tools = [
        {"name": "open_orders_bought(app, order_status)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹ä¹°å…¥çš„æŒ‡å®šçŠ¶æ€çš„è®¢å•åˆ—è¡¨ï¼Œä¾‹å¦‚å¾…ä»˜æ¬¾ã€å¾…æ”¶è´§ã€å¾…è¯„ä»·ç­‰ã€‚"},
        {"name": "open_orders_sold(app, order_status)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è‡ªå·±å”®å–çš„æŒ‡å®šçŠ¶æ€çš„è®¢å•åˆ—è¡¨ï¼Œä¾‹å¦‚å¾…ä»˜æ¬¾ã€å¾…æ”¶è´§ã€å¾…è¯„ä»·ç­‰ã€‚"},
        {"name": "open_orders_all_review(app)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹å¾…è¯„ä»·çŠ¶æ€çš„è®¢å•åˆ—è¡¨ï¼Œåœ¨ä¸æŒ‡å®šè´­ä¹°è¿˜æ˜¯å”®å–çš„è®¢å•æ—¶ï¼ŒåŠå…¨éƒ½è¦çœ‹æ—¶ä½¿ç”¨ã€‚"},
        {"name": "search_order(app, search_info, order_status)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æœç´¢è®¢å•"},
        {"name": "open_invoice_page(app, page_type)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æ‰“å¼€ä¸å‘ç¥¨ç›¸å…³çš„é¡µé¢"},
        {"name": "open_cart_content(app, filter_type)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è´­ç‰©è½¦/é‡‡è´­è½¦ï¼ˆé˜¿é‡Œå·´å·´çš„å«æ³•ï¼‰æŒ‡å®šç±»å‹çš„å•†å“"},
        {"name": "search_cart_content(app, search_info)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­æŸ¥çœ‹è´­ç‰©è½¦/é‡‡è´­è½¦ï¼ˆé˜¿é‡Œå·´å·´çš„å«æ³•ï¼‰æŸ¥æ‰¾å•†å“"},
        {"name": "open_customer_service(app)", "description": "åœ¨appåº”ç”¨ç¨‹åºä¸­è”ç³»å®¢æœ"},
        {"name": "sign_in(app, page_type)", "description": "åœ¨appç¨‹åºä¸­å®Œæˆæ¯æ—¥ç­¾åˆ°ï¼Œé¢†å–ç§¯åˆ†ã€é‡‘å¸ç­‰å¥–åŠ±çš„æ“ä½œ"},
        {"name": "open_favorite_goods(app, filter_type, order_type)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±ã€æƒ³è¦æˆ–å…³æ³¨å•†å“çš„é¡µé¢ï¼Œå¹¶æŒ‰ç…§æ¡ä»¶è¿›è¡Œç­›é€‰"},
        {"name": "open_favorite_stores(app, filter_type)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±æˆ–å…³æ³¨åº—é“ºçš„é¡µé¢ï¼Œå¹¶æŒ‰ç…§æ¡ä»¶è¿›è¡Œç­›é€‰"},
        {"name": "search_in_favorite_goods(app, search_info)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„ã€å–œçˆ±ã€æƒ³è¦æˆ–å…³æ³¨å•†å“çš„é¡µé¢ï¼Œå¹¶åœ¨å…¶ä¸­çš„æœç´¢æ ä¸­è¿›è¡Œæœç´¢"},
        {"name": "search_in_favorite_stores(app, search_info)", "description": "åœ¨appç¨‹åºä¸­æ‰“å¼€æ”¶è—çš„å–œçˆ±æˆ–å…³æ³¨åº—é“ºçš„é¡µé¢ï¼Œå¹¶åœ¨å…¶ä¸­çš„æœç´¢æ æœç´¢å•†å“"},
        {"name": "search_goods(app, search_info, order_type)", "description": "åœ¨appç¨‹åºä¸­ä¾æ®åç§°æœç´¢å•†å“ï¼Œå¯ä»¥æŒ‡å®šæœç´¢ç»“æœçš„æ’åºæ–¹å¼"},
        {"name": "search_stores(app, search_info, filter_type, order_type)", "description": "åœ¨appç¨‹åºä¸­ä¾æ®åç§°æœç´¢åº—é“ºï¼Œå¯ä»¥ä½¿ç”¨ç­›é€‰å™¨é™åˆ¶æœç´¢ç»“æœï¼Œä¹Ÿå¯ä»¥æŒ‡å®šæœç´¢ç»“æœçš„æ’åºæ–¹å¼"},
        {"name": "open_search_history(app)", "description": "æ‰“å¼€appç¨‹åºçš„æœç´¢å†å²ç•Œé¢"},
        {"name": "delete_search_history(app)", "description": "æ¸…é™¤appä¸­çš„æœç´¢å†å²"},
        {"name": "open_camera_search(app)", "description": "æ‰“å¼€appç¨‹åºçš„å›¾ç‰‡æœç´¢åŠŸèƒ½"},
        {"name": "open_logistics_receive(app, filter_type)", "description": "æ‰“å¼€æ˜¾ç¤ºå·²è´­å•†å“ä¿¡æ¯çš„ç•Œé¢ï¼ŒæŸ¥çœ‹ç›¸å…³ç‰©æµä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç‰©æµæƒ…å†µè¿›è¡Œç­›é€‰"},
        {"name": "open_logistics_send(app, filter_type)", "description": "æ‰“å¼€æ˜¾ç¤ºå·²å”®å•†å“ä¿¡æ¯çš„ç•Œé¢ï¼ŒæŸ¥çœ‹ç›¸å…³ç‰©æµä¿¡æ¯ï¼Œå¹¶æ ¹æ®ç‰©æµæƒ…å†µè¿›è¡Œç­›é€‰"},
        {"name": "open_express_delivery(app)", "description": "æ‰“å¼€appå¯„é€å¿«é€’çš„ç•Œé¢"},
        {"name": "open_app(app)", "description": "æ‰“å¼€æŒ‡å®šçš„åº”ç”¨ç¨‹åº"},
    ]
    return tools


# â˜…â˜…â˜… 2. å®šä¹‰åŒä¹‰è¯æ‰©å±•å‡½æ•°å’Œè¯­æ–™åº“æ„å»ºå‡½æ•° â˜…â˜…â˜…
def get_synonyms():
    return {
        "è´­ç‰©è½¦": ["é‡‡è´­è½¦"],
        "æ”¶è—": ["å–œæ¬¢", "æƒ³è¦", "å…³æ³¨", "æ”¶è—å¤¹"],
        # "æŸ¥": ["æ‰¾", "æœ", "æœç´¢", "æŸ¥è¯¢"], # <--- è¿™ä¸ªå¤ªæ¨¡ç³Šï¼Œå¯ä»¥è€ƒè™‘ç§»é™¤æˆ–ç»†åŒ–
        "æ‰¾è®¢å•": ["æŸ¥è®¢å•", "æœè®¢å•"],
        "çœ‹ç‰©æµ": ["æŸ¥å¿«é€’", "åŒ…è£¹è¿›åº¦"],
        "ä¹°çš„ä¸œè¥¿": ["æˆ‘ä¹°çš„", "æ”¶åˆ°çš„", "è´­ä¹°è®°å½•"],
        "å–çš„ä¸œè¥¿": ["æˆ‘å–çš„", "å‘å‡ºçš„", "å”®å‡ºè®°å½•"],
        "å‘ç¥¨": ["å¼€ç¥¨", "æŠ¥é”€å‡­è¯"],
    }

def build_corpus(data_df: pd.DataFrame, tool_definitions: list) -> list:
    print("--- æ­£åœ¨æ„å»ºå¢å¼ºè¯­æ–™åº“... ---")
    synonyms = get_synonyms()
    
    tool_text_aggregator = defaultdict(list)
    for _, row in data_df.iterrows():
        if row['available_tools']:
            tool_name = row['available_tools'][0]['name']
            tool_text_aggregator[tool_name].append(row['instruction_template'])
            tool_text_aggregator[tool_name].append(row['final_query'])
            
    all_tools_corpus = []
    for tool_def in tool_definitions:
        tool_name = tool_def['name']
        
        # åŸºç¡€æ–‡æœ¬ï¼šæè¿° + èšåˆçš„ç”¨æˆ·æŸ¥è¯¢
        description = tool_def.get('description', '')
        aggregated_queries = ' '.join(set(tool_text_aggregator.get(tool_name, [])))
        
        # åŒä¹‰è¯æ‰©å±•
        synonym_expansion = []
        for word, syns in synonyms.items():
            if word in description:
                synonym_expansion.extend(syns)
        
        synonym_text = ' '.join(set(synonym_expansion))
        
        # æ„å»ºæœ€ç»ˆæ–‡æ¡£
        document = f"è¿™æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œå®ƒçš„åŠŸèƒ½æ˜¯ {description}ã€‚åŒä¹‰è¯å‚è€ƒ: {synonym_text}ã€‚ç”¨æˆ·å¯èƒ½ä¼šè¿™æ ·è¯´ï¼š{aggregated_queries}"

        # document += f" åŒä¹‰è¯å‚è€ƒ: {synonym_text}ã€‚ç”¨æˆ·å¯èƒ½ä¼šè¿™æ ·è¯´ï¼š{aggregated_queries}"
        all_tools_corpus.append(document)
        
    print(f"è¯­æ–™åº“æ„å»ºå®Œæˆï¼Œå…± {len(all_tools_corpus)} ä¸ªå·¥å…·ã€‚\n")
    return all_tools_corpus

# --- ä¸»ç¨‹åº ---
def main():
    # --- 0. é…ç½®åŒºåŸŸ ---
    annotated_data_file_path = r'/home/workspace/lgq/shop/data/corrected_data.csv' # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
    K_VALUES = [1, 2, 3, 4, 5]
    NUM_ERROR_EXAMPLES_TO_PRINT = 10 # æ‰“å°å¤šå°‘ä¸ªé”™è¯¯æ¡ˆä¾‹

    # --- 1. æ•°æ®åŠ è½½ä¸åˆ’åˆ† ---
    print("--- æ­¥éª¤ 1: åŠ è½½å·²æ ‡æ³¨æ•°æ®å¹¶åˆ’åˆ† ---")
    try:
        required_columns = ['instruction_template', 'final_query', 'is_train', 'available_tools']
        data_df = pd.read_csv(annotated_data_file_path, usecols=required_columns)
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–æ–‡ä»¶ '{annotated_data_file_path}' å¤±è´¥. {e}")
        return
    
    def parse_tools(tool_string):
        try: return ast.literal_eval(tool_string)
        except (ValueError, SyntaxError): return []
    data_df['available_tools'] = data_df['available_tools'].apply(parse_tools)
    
    full_train_df = data_df[data_df['is_train'] == 0].copy()
    test_df = data_df[data_df['is_train'] == 1].copy()
    
    # â˜…â˜…â˜… 3. ä» full_train_df ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†ï¼Œç”¨äºå‚æ•°æœç´¢ â˜…â˜…â˜…
    if len(full_train_df) > 10: # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®åˆ’åˆ†
        train_df, val_df = train_test_split(full_train_df, test_size=0.2, random_state=42)
    else:
        train_df, val_df = full_train_df, full_train_df # æ•°æ®å¤ªå°‘ï¼Œä¸åˆ’åˆ†

    print(f"æ•°æ®åˆ’åˆ†å®Œæˆï¼šè®­ç»ƒé›† {len(train_df)} æ¡ï¼ŒéªŒè¯é›† {len(val_df)} æ¡ï¼Œæµ‹è¯•é›† {len(test_df)} æ¡ã€‚\n")

    # --- 2. å‚æ•°æœç´¢ (Grid Search) ---
    print("--- æ­¥éª¤ 2: åœ¨éªŒè¯é›†ä¸Šæœç´¢æœ€ä½³ BM25 å‚æ•° ---")
    all_tools_definitions = get_exact_tool_definitions()
    
    # ä½¿ç”¨è®­ç»ƒé›†æ„å»ºè¯­æ–™åº“
    corpus_for_tuning = build_corpus(train_df, all_tools_definitions)
    
    best_score = -1
    best_params = {'k1': 1.5, 'b': 0.75} # é»˜è®¤å€¼
    k1_range = [1.2, 1.5, 1.8, 2.0]
    b_range = [0.6, 0.75, 0.9]

    for k1, b in tqdm(list(itertools.product(k1_range, b_range)), desc="å‚æ•°æœç´¢ä¸­"):
        temp_retriever = ToolRetriever(corpus_for_tuning, all_tools_definitions, k1=k1, b=b)
        recalls_at_1 = []
        for _, row in val_df.iterrows():
            query = row['final_query']
            gt = row['available_tools']
            retrieved, _ = temp_retriever.retrieve_with_scores(query, top_k=1)
            recalls_at_1.append(calculate_recall_at_k(retrieved, gt, k=1))
        
        current_score = np.mean(recalls_at_1)
        if current_score > best_score:
            best_score = current_score
            best_params = {'k1': k1, 'b': b}

    print(f"å‚æ•°æœç´¢å®Œæˆï¼æœ€ä½³å‚æ•°: {best_params} (åœ¨éªŒè¯é›†ä¸Šçš„ Recall@1: {best_score:.4f})\n")

    # --- 3. æ„å»ºæœ€ç»ˆå¬å›å™¨ ---
    print("--- æ­¥éª¤ 3: ä½¿ç”¨æœ€ä½³å‚æ•°å’Œå…¨éƒ¨è®­ç»ƒæ•°æ®æ„å»ºæœ€ç»ˆå¬å›å™¨ ---")
    # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ® (train + val) æ„å»ºæœ€ç»ˆè¯­æ–™åº“
    final_corpus = build_corpus(full_train_df, all_tools_definitions)
    # ä½¿ç”¨æœ€ä½³å‚æ•°æ„å»ºæœ€ç»ˆå¬å›å™¨
    retriever = ToolRetriever(final_corpus, all_tools_definitions, k1=best_params['k1'], b=best_params['b'])
    print("æœ€ç»ˆå¬å›å™¨æ„å»ºå®Œæˆã€‚\n")

    # --- 4. åœ¨æµ‹è¯•é›†ä¸Šè¯„æµ‹ ---
    print(f"--- æ­¥éª¤ 4: å¼€å§‹åœ¨ {len(test_df)} ä¸ªæµ‹è¯•é›†æ ·æœ¬ä¸Šè¿›è¡Œè¯„æµ‹ ---")
    results = {
        'Recall@K': {k: [] for k in K_VALUES},
        'HR@K': {k: [] for k in K_VALUES},
        'MAP@K': {k: [] for k in K_VALUES},
        'MRR@K': {k: [] for k in K_VALUES},
        'NDCG@K': {k: [] for k in K_VALUES},
        'COMP@K': {k: [] for k in K_VALUES},
        'AUC': [],
        'processing_time': []
    }
    error_cases = []

    for i, (_, row) in enumerate(tqdm(test_df.iterrows(), total=len(test_df), desc="æµ‹è¯•é›†è¯„æµ‹ä¸­")):
        query = row['final_query']
        ground_truth = row['available_tools']
        
        start_time = time.perf_counter()
        retrieved, all_scores = retriever.retrieve_with_scores(query, top_k=max(K_VALUES))
        duration = time.perf_counter() - start_time
        
        results['processing_time'].append(duration)
        results['AUC'].append(calculate_auc_for_query(all_scores, all_tools_definitions, ground_truth))

        for k in K_VALUES:
            results['Recall@K'][k].append(calculate_recall_at_k(retrieved, ground_truth, k))
            # ... (æ·»åŠ å…¶ä»–æŒ‡æ ‡)
            results['HR@K'][k].append(calculate_hit_ratio_at_k(retrieved, ground_truth, k))
            results['MAP@K'][k].append(calculate_average_precision_at_k(retrieved, ground_truth, k))
            results['MRR@K'][k].append(calculate_mrr_at_k(retrieved, ground_truth, k))
            results['NDCG@K'][k].append(calculate_ndcg_at_k(retrieved, ground_truth, k))
            results['COMP@K'][k].append(calculate_completeness_at_k(retrieved, ground_truth, k))
        
        # â˜…â˜…â˜… 4. é”™è¯¯åˆ†æï¼šè®°å½• Top-1 é¢„æµ‹å¤±è´¥çš„æ ·æœ¬ â˜…â˜…â˜…
        is_top1_correct = calculate_recall_at_k(retrieved, ground_truth, k=1) >= 1.0
        if not is_top1_correct:
            gt_name = _get_tool_names(ground_truth).pop() if ground_truth else "N/A"
            pred_name_top1 = retrieved[0].get('name') if retrieved else "N/A"
            error_cases.append({
                "Query": query,
                "Ground Truth": gt_name,
                "Prediction@1": pred_name_top1,
                "Prediction@5": [r.get('name') for r in retrieved]
            })

    # --- 5. æ±‡æ€»å¹¶æŠ¥å‘Šç»“æœ ---
    print("\n\n--- æ­¥éª¤ 5: è¯„æµ‹ç»“æœæŠ¥å‘Š ---")
    # (æŠ¥å‘ŠæŒ‡æ ‡éƒ¨åˆ†ä»£ç ä¸å˜)
    final_scores = {}
    for metric, vals in results.items():
        if metric == 'AUC': final_scores['AUC'] = np.mean(vals)
        elif metric == 'processing_time': continue
        else: final_scores[metric] = {f"@{k}": np.mean(v) for k, v in vals.items()}
    report_df = pd.DataFrame({ m: final_scores[m] for m in ['Recall@K', 'HR@K', 'MAP@K', 'MRR@K', 'NDCG@K', 'COMP@K']}).T
    report_df.columns = [f"@{k}" for k in K_VALUES]
    print("BM25 å¬å›æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„æµ‹ç»“æœ:")
    print("-" * 50)
    print(report_df.to_string(formatters={col: '{:.4f}'.format for col in report_df.columns}))
    print(f"\n**AUC (å…¨é‡æ’åº ROC AUC)**: {final_scores['AUC']:.4f}")
    print("-" * 50)
    
    # (æŠ¥å‘Šæ€§èƒ½éƒ¨åˆ†ä»£ç ä¸å˜)
    total_time, avg_time_ms = np.sum(results['processing_time']), np.mean(results['processing_time']) * 1000
    qps = len(test_df) / total_time if total_time > 0 else 0
    print("\næ€§èƒ½è¯„æµ‹:")
    print("-" * 50)
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(test_df)} æ¡")
    print(f"æ€»è€—æ—¶: {total_time:.4f} ç§’, å¹³å‡æ¯æ¡è€—æ—¶: {avg_time_ms:.4f} æ¯«ç§’, QPS: {qps:.2f}")
    print("-" * 50)
    
    # â˜…â˜…â˜… 5. æ‰“å°é”™è¯¯åˆ†ææŠ¥å‘Š â˜…â˜…â˜…
    print(f"\n\n--- æ­¥éª¤ 6: Top-1 é”™è¯¯æ¡ˆä¾‹åˆ†æ (å…± {len(error_cases)} ä¸ªé”™è¯¯) ---")
    if not error_cases:
        print("ğŸ‰ æ­å–œï¼åœ¨æµ‹è¯•é›†ä¸Šæ²¡æœ‰å‘ç° Top-1 é”™è¯¯æ¡ˆä¾‹ï¼")
    else:
        for i, case in enumerate(error_cases[:NUM_ERROR_EXAMPLES_TO_PRINT]):
            print(f"\n--- é”™è¯¯æ¡ˆä¾‹ {i+1}/{len(error_cases)} ---")
            print(f"  [æŸ¥è¯¢ Query]: {case['Query']}")
            print(f"  [çœŸå®å·¥å…· Ground Truth]: {case['Ground Truth']}")
            print(f"  [é¢„æµ‹å·¥å…· Prediction@1]: {case['Prediction@1']}")
            print(f"  [é¢„æµ‹å·¥å…· Prediction@5]: {case['Prediction@5']}")
        if len(error_cases) > NUM_ERROR_EXAMPLES_TO_PRINT:
            print(f"\n... (ä»…æ˜¾ç¤ºå‰ {NUM_ERROR_EXAMPLES_TO_PRINT} ä¸ªé”™è¯¯æ¡ˆä¾‹) ...")
    print("-" * 50)


if __name__ == "__main__":
    main()